{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notbook show an example the uilization of TRAILDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from dataloader import TRAILDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One columns as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "csv_file = \"/home/schmitt/Documents/Trail/Processed_data.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "df_train = df[df[\"Year\"] == 2017]\n",
    "df_test = df[df[\"Year\"] == 2018]\n",
    "\n",
    "\n",
    "input_size = 70\n",
    "# columns_input = [\"Total Load\", \"Hour\", \"Lockdown\"]  # Replace with your columns of interest\n",
    "columns_input = [\"Total Load\"]  # Replace with your columns of interest\n",
    "column_to_predict = \"Total Load\"\n",
    "column_to_normalize = [\"Total Load\"]\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = TRAILDataset(df_train, input_size, columns_input, column_to_predict, column_to_normalize)\n",
    "# scaler = train_dataset.get_scaler()\n",
    "predict_scaler = train_dataset.get_predict_scaler()\n",
    "print(\"train dataset loaded\")\n",
    "print(column_to_normalize)\n",
    "test_dataset = TRAILDataset(df_test, input_size, columns_input, column_to_predict, column_to_normalize, predict_scaler=predict_scaler)\n",
    "print(\"test dataset loaded\")\n",
    "\n",
    "# Create d\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Example of iterating over the DataLoader\n",
    "for batch in train_loader:\n",
    "    input, ground_truth = batch\n",
    "    print(\"input\", input.shape)\n",
    "    # print(\"\\n Input: \\n\", input)\n",
    "\n",
    "    output = input[:, 0, :4]\n",
    "    print(output.shape)\n",
    "\n",
    "    dernomalized_output = train_dataset.dernormalize(output)\n",
    "    # print(\"\\n dernomalized_output: \\n\", dernomalized_output)\n",
    "    # print(dernomalized_output[0, :])\n",
    "    # print(\"\\n ground_truth normalized: \\n\", ground_truth)\n",
    "    print(\"\\n ground_truth denormalized: \\n\", train_dataset.dernormalize(ground_truth))\n",
    "\n",
    "    break\n",
    "for batch in test_loader:\n",
    "    input, ground_truth = batch\n",
    "    # print(\"\\n Input: \\n\", input)\n",
    "\n",
    "    output = input[:, 0, :4]\n",
    "    dernomalized_output = test_dataset.dernormalize(output)\n",
    "    # print(\"\\n dernomalized_output: \\n\", dernomalized_output)\n",
    "    # print(dernomalized_output[0, :])\n",
    "    # print(\"\\n ground_truth normalized: \\n\", ground_truth)\n",
    "    print(\"\\n ground_truth denormalized: \\n\", test_dataset.dernormalize(ground_truth))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several columns as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "csv_file = \"/home/schmitt/Documents/Trail/Processed_data.csv\"  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "df_train = df[df[\"Year\"] == 2017]\n",
    "df_test = df[df[\"Year\"] == 2018]\n",
    "\n",
    "\n",
    "input_size = 5\n",
    "columns_input = [\"Total Load\", \"Hour\", \"Lockdown\"]  # Replace with your columns of interest\n",
    "column_to_predict = \"Total Load\"\n",
    "column_to_normalize = [\"Total Load\", \"Hour\", \"Lockdown\"]\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = TRAILDataset(df_train, input_size, columns_input, column_to_predict, column_to_normalize)\n",
    "scaler = train_dataset.get_scaler()\n",
    "predict_scaler = train_dataset.get_predict_scaler()\n",
    "print(\"train dataset loaded\")\n",
    "print(column_to_normalize)\n",
    "test_dataset = TRAILDataset(df_test, input_size, columns_input, column_to_predict, column_to_normalize, scaler=scaler, predict_scaler=predict_scaler)\n",
    "print(\"test dataset loaded\")\n",
    "\n",
    "# Create d\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Example of iterating over the DataLoader\n",
    "for batch in train_loader:\n",
    "    input, ground_truth = batch\n",
    "    # print(\"\\n Input: \\n\", input)\n",
    "\n",
    "    output = input[:, 0, :4]\n",
    "    dernomalized_output = train_dataset.dernormalize(output)\n",
    "    # print(\"\\n dernomalized_output: \\n\", dernomalized_output)\n",
    "    # print(dernomalized_output[0, :])\n",
    "    # print(\"\\n ground_truth normalized: \\n\", ground_truth)\n",
    "    print(\"\\n ground_truth denormalized: \\n\", train_dataset.dernormalize(ground_truth))\n",
    "\n",
    "    break\n",
    "for batch in test_loader:\n",
    "    input, ground_truth = batch\n",
    "    # print(\"\\n Input: \\n\", input)\n",
    "\n",
    "    output = input[:, 0, :4]\n",
    "    dernomalized_output = test_dataset.dernormalize(output)\n",
    "    # print(\"\\n dernomalized_output: \\n\", dernomalized_output)\n",
    "    # print(dernomalized_output[0, :])\n",
    "    # print(\"\\n ground_truth normalized: \\n\", ground_truth)\n",
    "    print(\"\\n ground_truth denormalized: \\n\", test_dataset.dernormalize(ground_truth))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_scaler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
